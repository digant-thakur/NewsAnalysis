{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #.+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"asanew.csv\")\n",
    "#df.groupby('code').nunique()\n",
    "#labels = df['code']\n",
    "#text = df['text']\n",
    "#labels.nunique()\n",
    "#text\n",
    "#text1 = text.apply(clean_text)\n",
    "#text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 by mouth daily</td>\n",
       "      <td>1QD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 by mouth every day</td>\n",
       "      <td>1QD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 daily</td>\n",
       "      <td>1QD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 (one)  Tablet by mouth daily</td>\n",
       "      <td>1QD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 (one) Tablet, Oral, daily</td>\n",
       "      <td>1QD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text                  code\n",
       "0                1 by mouth daily  1QD                 \n",
       "1            1 by mouth every day  1QD                 \n",
       "2                         1 daily  1QD                 \n",
       "3  1 (one)  Tablet by mouth daily  1QD                 \n",
       "4     1 (one) Tablet, Oral, daily  1QD                 "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "data = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(20000, 100, input_length=50))\n",
    "model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1QD                 \n",
       "1       1QD                 \n",
       "2       1QD                 \n",
       "3       1QD                 \n",
       "4       1QD                 \n",
       "5       1QD                 \n",
       "6       1QD                 \n",
       "7       1QD                 \n",
       "8       1QD                 \n",
       "9       1QD                 \n",
       "10      1QD                 \n",
       "11      1QD                 \n",
       "12      1QD                 \n",
       "13      1QD                 \n",
       "14      1QD                 \n",
       "15      1QD                 \n",
       "16      1QD                 \n",
       "17      1QD                 \n",
       "18      1QD                 \n",
       "19      1QD                 \n",
       "20      1QD                 \n",
       "21      1QD                 \n",
       "22      1QD                 \n",
       "23      1QD                 \n",
       "24      1QD                 \n",
       "25      1QD                 \n",
       "26      1QD                 \n",
       "27      1QD                 \n",
       "28      1QD                 \n",
       "29      1QD                 \n",
       "                ...         \n",
       "3970    UAD                 \n",
       "3971    1QD30D              \n",
       "3972    1QD                 \n",
       "3973    1QD                 \n",
       "3974    1QW30D              \n",
       "3975    1QD                 \n",
       "3976    3QD                 \n",
       "3977    1TID                \n",
       "3978    AABID               \n",
       "3979    1HSUD               \n",
       "3980    1Q12H10D            \n",
       "3981    1QD90D              \n",
       "3982    2Q4HPRN             \n",
       "3983    1BID10D             \n",
       "3984    1BID5D              \n",
       "3985    NITRO               \n",
       "3986    1HS                 \n",
       "3987    1TID10D             \n",
       "3988    1QD90D              \n",
       "3989    1QD                 \n",
       "3990    1HSPRN              \n",
       "3991    1TID10D             \n",
       "3992    1QD                 \n",
       "3993    1QD                 \n",
       "3994    1QD                 \n",
       "3995    2TID                \n",
       "3996    1QD                 \n",
       "3997    1BID                \n",
       "3998    1BID10D             \n",
       "3999    1QD                 \n",
       "Name: code, Length: 4000, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2400 samples, validate on 1600 samples\n",
      "Epoch 1/3\n",
      "2400/2400 [==============================] - 13s 5ms/step - loss: -1507.7911 - acc: 4.1667e-04 - val_loss: -2159.3362 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "2400/2400 [==============================] - 11s 5ms/step - loss: -2175.9163 - acc: 4.1667e-04 - val_loss: -2159.3362 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "2400/2400 [==============================] - 12s 5ms/step - loss: -2175.9163 - acc: 4.1667e-04 - val_loss: -2159.3362 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d9a8491e10>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(data, np.array(labels), validation_split=0.4, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_model():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(Embedding(vocabulary_size, 100, input_length=50))\n",
    "    model_conv.add(Dropout(0.2))\n",
    "    model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(LSTM(100))\n",
    "    model_conv.add(Dense(1, activation='sigmoid'))\n",
    "    model_conv.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2400 samples, validate on 1600 samples\n",
      "Epoch 1/3\n",
      "2400/2400 [==============================] - 8s 3ms/step - loss: -1487.2725 - acc: 4.1667e-04 - val_loss: -2159.3362 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "2400/2400 [==============================] - 6s 3ms/step - loss: -2175.9163 - acc: 4.1667e-04 - val_loss: -2159.3362 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "2400/2400 [==============================] - 6s 2ms/step - loss: -2175.9163 - acc: 4.1667e-04 - val_loss: -2159.3362 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d9a8698d68>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv = create_conv_model()\n",
    "model_conv.fit(data, np.array(labels), validation_split=0.4, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(100))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2400 samples, validate on 1600 samples\n",
      "Epoch 1/3\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: -1650.3515 - acc: 4.1667e-04 - val_loss: -2159.3362 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "2400/2400 [==============================] - 3s 1ms/step - loss: -2175.9163 - acc: 4.1667e-04 - val_loss: -2159.3362 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "2400/2400 [==============================] - 4s 2ms/step - loss: -2175.9163 - acc: 4.1667e-04 - val_loss: -2159.3362 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d9a86647b8>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove.fit(data, np.array(labels), validation_split=0.4, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
